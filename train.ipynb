{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'come'\n",
    "\n",
    "data = np.load(f'seq_{action}_1681635479.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(834, 30, 99)\n",
      "(834,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = np.zeros(data.shape[0])\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=1)\n",
    "y_data.shape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 30, 99) (750, 1)\n",
      "(84, 30, 99) (84, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 64)                41984     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,097\n",
      "Trainable params: 44,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 64)                41984     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,097\n",
      "Trainable params: 44,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7895 - acc: 0.8707\n",
      "Epoch 1: val_acc improved from -inf to 1.00000, saving model to model.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.7413 - acc: 0.8787 - val_loss: 0.0024 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 4.3499e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 1.4294e-06 - acc: 1.0000\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 1.2821e-06 - acc: 1.0000 - val_loss: 1.6069e-11 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 5.5420e-05 - acc: 1.0000\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 5.1376e-05 - acc: 1.0000 - val_loss: 6.7989e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 2.0452e-05 - acc: 1.0000\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 1.8405e-05 - acc: 1.0000 - val_loss: 2.1269e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 3.8973e-06 - acc: 1.0000\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 3.4984e-06 - acc: 1.0000 - val_loss: 1.2713e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 1.4986e-05 - acc: 1.0000\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 1.5835e-05 - acc: 1.0000 - val_loss: 1.2278e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.7112e-05 - acc: 1.0000\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 1.6811e-05 - acc: 1.0000 - val_loss: 2.4542e-06 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 2.1343e-05 - acc: 1.0000\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 1.9123e-05 - acc: 1.0000 - val_loss: 1.9160e-08 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0043 - acc: 0.9985\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 8.7372e-35 - acc: 1.0000\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 7.8285e-35 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 21: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 22: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 24: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 25: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 26: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 27: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 28: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 29: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 30: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 31: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 32: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 33: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 34: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 35: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 36: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 37: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 38: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 39: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 40: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 41: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 42: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 43: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 45: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 46: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 47: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 48: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 49: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 50: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 51: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 52: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 53: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 54: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 55: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 56: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 57: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 58: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 59: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 60: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 61: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 62: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 63: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 64: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 65: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 66: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 67: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 68: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 69: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 70: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 71: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 72: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 73: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 74: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 75: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 76: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 77: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 78: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 79: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 80: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 81: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 82: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 83: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 84: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 85: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 86: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 87: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 88: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 89: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 90/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 90: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 91/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 91: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 92/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 92: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 93/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 93: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 94/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 94: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 95/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 95: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 96/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 96: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 97/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 97: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 98/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 98: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 99: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 100: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 101: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 102: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 103/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 103: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 104/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 104: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 105/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 105: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 106/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 106: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 107/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 107: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 108/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 108: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 109/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 109: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 110/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 110: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 111/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 111: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 112/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 112: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 113/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 113: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 114/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 114: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 115/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 115: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 116/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 116: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 117/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 117: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 118/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 118: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 119/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 119: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 120/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 120: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 121/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 121: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 122/200\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 122: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 123/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 123: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 124/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 124: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 125/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 125: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 126/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 126: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 127/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 127: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 128/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 128: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 129/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 129: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 130/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 130: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 131/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 131: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 132/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 132: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 133/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 133: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 134/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 134: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 135/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 135: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 136/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 136: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 137/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 137: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 138/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 138: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 139/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 139: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 140/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 140: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 141/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 141: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 142/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 142: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 143/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 143: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 144/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 144: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 145/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 145: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 146/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 146: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 147/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 147: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 148/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 148: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 149/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 149: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 150/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 150: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 151/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 151: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 152/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 152: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 153/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 153: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 154/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 154: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 155/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 155: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 156/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 156: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 157/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 157: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 158/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 158: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 159/200\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 159: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 160/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 160: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 161/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 161: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 162/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 162: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 163/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 163: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 164/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 164: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 165/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 165: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 166/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 166: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 167/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 167: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 168/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 168: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 169/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 169: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 170/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 170: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 171/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 171: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 172/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 172: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 173/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 173: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 174/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 174: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 175/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 175: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 176/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 176: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 177/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 177: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 178/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 178: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 179/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 179: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 180/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 180: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 181/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 181: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 182/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 182: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 183/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 183: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 184/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 184: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 185/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 185: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 186/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 186: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 187/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 187: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 188/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 188: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 189/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 189: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 190/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 190: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 191/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 191: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 192/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 192: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 193/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 193: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 194/200\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 194: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 195/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 195: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 196/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 196: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 197/200\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 197: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 198/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 198: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 199/200\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 199: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 200/200\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 200: val_acc did not improve from 1.00000\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "plt.show()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "confusion_matrix(y_val > 0.5 , y_pred > 0.5)\n",
    "\n",
    "if len(y_pred) == 1:\n",
    "    conf = y_pred[0]\n",
    "else:\n",
    "    i_pred = int(np.argmax(y_pred))\n",
    "    conf = y_pred[i_pred]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
