{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 15, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 'come'\n",
    "\n",
    "data = np.load(f'dataset/seq_{action}_1681485672.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 15)\n",
      "(404,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = np.zeros(data.shape[0])\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=1)\n",
    "y_data.shape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323, 15) (323, 1)\n",
      "(81, 15) (81, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 15, 64)            64        \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,201\n",
      "Trainable params: 35,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000 \n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to models\\regression_model.h5\n",
      "11/11 [==============================] - 2s 33ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 2/200\n",
      " 7/11 [==================>...........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 4/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 5: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 6: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 7: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 8/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 8: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 9/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 9: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 10/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 10: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 11: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 12: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 13: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 14: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 15/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 15: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 16: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 17: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      " 5/11 [============>.................] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 18: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 19: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 20/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 20: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 21/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 21: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 22: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 23/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 23: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 24: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 25/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 25: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 26: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 27: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 28: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 29: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 30: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 31/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 31: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 32: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 33: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 34/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 34: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 35: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 36: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 37/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 37: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 38: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 39: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 40: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 41/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 41: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 42: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 43: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 44/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 44: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 45: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 46: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 47: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 48: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 49: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 50: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 51: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 52: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 53: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 54: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 55: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 56: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 57: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 58: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 59: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 60: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 61: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 62: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 64: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 65: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 66: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 67: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 68: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 69: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 70: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 71: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 72: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 73: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 74: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 76: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 77: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 78: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 79: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 80: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 81: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 82: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 83: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 84: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 85: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 86: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 87: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      " 6/11 [===============>..............] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 88: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 89: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 90/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 90: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 91/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 91: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 92/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 92: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 93/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 93: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 94/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 94: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 95/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 95: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 96/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 96: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 97/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 97: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 98/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 98: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 99: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 100: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 101: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 102: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 103/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 103: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 104/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 104: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 105/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 105: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 106/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 106: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 107: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 108/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 108: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 109/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 109: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 110/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 110: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 111/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 111: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 112/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 112: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 113/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 113: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 114/200\n",
      " 4/11 [=========>....................] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 114: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 115/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 115: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 116/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 116: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 117/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 117: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 118/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 118: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 119/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 119: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 120/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 120: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 121/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 121: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 122/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 122: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 123/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 123: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 124/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 124: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 125/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 125: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 126/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 126: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 127/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 127: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 128/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 128: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 129/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 129: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 130/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 130: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 131/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 131: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 132/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 132: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 133/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 133: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 134/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 134: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 135/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 135: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 136/200\n",
      " 6/11 [===============>..............] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 136: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 137/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 137: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 138/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 138: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 139/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 139: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 140/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 140: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 141/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 141: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 142/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 142: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 143/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 143: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 144/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 144: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 145/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 145: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 146/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 146: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 147/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 147: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 148/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 148: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 149/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 149: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 150/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 151/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 152/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 153/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 154/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 155/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 155: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 156/200\n",
      " 5/11 [============>.................] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 156: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 157/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 157: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 158/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 159/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 160/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 160: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 161/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 161: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 162/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 162: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 163/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 163: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 164/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 165/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 166/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 167/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 167: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 168/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 169/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 170/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 171/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 172/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 173/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 174/200\n",
      " 7/11 [==================>...........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 174: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 175/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 175: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 176/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 176: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 177/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 177: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 178/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 178: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 179/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 179: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 180/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 181/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 182/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 182: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 183/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 183: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 184/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 185/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 186/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 187/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 187: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 188/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 189/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 189: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 190/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 191/200\n",
      " 4/11 [=========>....................] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 191: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 192/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 192: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 193/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 193: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 194/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 194: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 195/200\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 195: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 196/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 196: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 197/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 197: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 198/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 198: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 199/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 199: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 200/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 200: val_loss did not improve from 0.00000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1215 into shape (15,99)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mregression_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m x_val_reshaped \u001b[39m=\u001b[39m x_val\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m15\u001b[39;49m, \u001b[39m99\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_val_reshaped, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1215 into shape (15,99)"
     ]
    }
   ],
   "source": [
    "plt.show()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "confusion_matrix(y_val > 0.5 , y_pred > 0.5)\n",
    "\n",
    "if len(y_pred) == 1:\n",
    "    conf = y_pred[0]\n",
    "else:\n",
    "    i_pred = int(np.argmax(y_pred))\n",
    "    conf = y_pred[i_pred]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
